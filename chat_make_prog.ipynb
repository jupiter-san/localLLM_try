{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d45ffbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['LANG', 'input_requests'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['LANG'], input_types={}, partial_variables={}, template='You are an excellent programming assistant.Please output in the programming language specified in {LANG}.Please do not forget to include import statements so that you can copy and paste and execute immediately.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_requests'], input_types={}, partial_variables={}, template='{input_requests}'), additional_kwargs={})]\n",
      "```python\n",
      "from transformers import pipeline\n",
      "\n",
      "def chat_with_gemma3():\n",
      "    \"\"\"\n",
      "    Gemma3とチャットするための簡単なプログラムです。\n",
      "    \"\"\"\n",
      "\n",
      "    # モデルのロード\n",
      "    generator = pipeline(\"text-generation\", model=\"gemma3:7b\")\n",
      "\n",
      "    while True:\n",
      "        # ユーザーからの入力を取得\n",
      "        user_input = input(\"You: \")\n",
      "\n",
      "        # プロンプトの作成\n",
      "        prompt = f\"You: {user_input}\\nGemma3:\"\n",
      "\n",
      "        # テキスト生成\n",
      "        response = generator(prompt, max_length=200, num_return_sequences=1)[0]['generated_text']\n",
      "\n",
      "        # Gemma3の応答を抽出\n",
      "        gemma_response = response.split(\"Gemma3:\")[1].strip()\n",
      "\n",
      "        # 応答の出力\n",
      "        print(f\"Gemma3: {gemma_response}\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    chat_with_gemma3()\n",
      "```\n",
      "\n",
      "**説明:**\n",
      "\n",
      "1.  **`import transformers`**:  `transformers`ライブラリをインポートします。これは、Hugging Faceのモデルを簡単に使用するためのライブラリです。\n",
      "2.  **`chat_with_gemma3()` 関数**:\n",
      "    *   **`generator = pipeline(\"text-generation\", model=\"gemma3:7b\")`**:  `pipeline`関数を使用して、テキスト生成パイプラインを構築します。\n",
      "        *   `\"text-generation\"`:  テキスト生成タスクを指定します。\n",
      "        *   `model=\"gemma3:7b\"`:  使用するモデルを指定します。  `gemma3:7b` は、Gemma3の7Bパラメータモデルを指します。  モデルのダウンロードには時間がかかる場合があります。\n",
      "    *   **`while True:`**:  無限ループを作成し、ユーザーがチャットを終了するまで継続的に入力を受け付けます。\n",
      "    *   **`user_input = input(\"You: \")`**:  ユーザーからの入力を取得し、\"You: \"というプロンプトの後に表示されます。\n",
      "    *   **`prompt = f\"You: {user_input}\\nGemma3:\"`**:  ユーザーの入力とGemma3の応答を組み合わせて、プロンプト文字列を作成します。\n",
      "    *   **`response = generator(prompt, max_length=200, num_return_sequences=1)[0]['generated_text']`**:  `generator`パイプラインを使用してテキストを生成します。\n",
      "        *   `prompt`:  生成するテキストのプロンプト。\n",
      "        *   `max_length=200`:  生成するテキストの最大長を200トークンに設定します。\n",
      "        *   `num_return_sequences=1`:  生成するテキストの数を1に設定します。\n",
      "        *   `[0]['generated_text']`:  生成されたテキストからGemma3の応答を抽出します。\n",
      "    *   **`gemma_response = response.split(\"Gemma3:\")[1].strip()`**: Gemma3の応答を抽出します。\n",
      "    *   **`print(f\"Gemma3: {gemma_response}\")`**: Gemma3の応答をコンソールに出力します。\n",
      "3.  **`if __name__ == \"__main__\":`**:  このスクリプトが直接実行された場合にのみ、`chat_with_gemma3()`関数を呼び出します。\n",
      "\n",
      "**実行方法:**\n",
      "\n",
      "1.  **必要なライブラリのインストール**:\n",
      "    ```bash\n",
      "    pip install transformers\n",
      "    ```\n",
      "2.  **スクリプトの実行**:\n",
      "    ```bash\n",
      "    python your_script_name.py\n",
      "    ```\n",
      "    ( `your_script_name.py` は、スクリプトのファイル名に置き換えてください。)\n",
      "\n",
      "**注意点:**\n",
      "\n",
      "*   **モデルのダウンロード**:  このスクリプトを実行する前に、`gemma3:7b` モデルをダウンロードする必要があります。  モデルのダウンロードには時間がかかる場合があります。\n",
      "*   **GPUの使用**:  GPUが利用可能な場合は、テキスト生成が高速化されます。  `transformers`ライブラリは、GPUを自動的に使用するように設定されます。\n",
      "*   **応答の長さ**:  `max_length`パラメータを調整して、生成されるテキストの長さを変更できます。\n",
      "*   **プロンプトの調整**:  プロンプトを調整して、Gemma3の応答をより適切に誘導できます。\n",
      "*   **モデルのバージョン**:  `gemma3:7b` は、Gemma3の特定のバージョンを指します。  Hugging Face Model Hubで利用可能な最新のモデルを使用するように変更する必要がある場合があります。\n",
      "*   **エラー処理**:  このコードにはエラー処理が含まれていません。  より堅牢なプログラムを作成するには、エラー処理を追加することをお勧めします。\n",
      "*   **メモリ**:  大規模なモデルを使用しているため、十分なメモリが必要です。  メモリ不足エラーが発生した場合は、より小さなモデルを使用するか、より多くのメモリを搭載したマシンを使用する必要があります。\n",
      "\n",
      "processing time: 0:01:44.886746\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "dt_start = datetime.datetime.now()  # 開始時刻を取得\n",
    "\n",
    "# see https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollama: Add base_url and seed parameters #24735\n",
    "# https://github.com/langchain-ai/langchain/pull/24735\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:latest\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "prompt =  ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an excellent programming assistant.Please output in the programming language specified in {LANG}.Please do not forget to include import statements so that you can copy and paste and execute immediately.\",\n",
    "        ),\n",
    "        (\"human\", \"{input_requests}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print (prompt)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "ai_msg = chain.invoke(\n",
    "    {\n",
    "        \"input_requests\": \"gemma3とチャットするプログラムを書いてほしい。\",\n",
    "        \"LANG\": \"python\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# contentのみ抽出して表示\n",
    "print(ai_msg.content)\n",
    "\n",
    "dt_end = datetime.datetime.now()  # 終了時刻を取得\n",
    "dt_diff = dt_end - dt_start # 終了時刻から開始時刻を減算する\n",
    "\n",
    "print(\"processing time:\",dt_diff)  # 処理にかかった時間データを使用\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
